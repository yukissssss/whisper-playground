"""
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ—¥æœ¬èªæ–‡å­—èµ·ã“ã—ãƒ„ãƒ¼ãƒ« â€“ medium ãƒ¢ãƒ‡ãƒ«ç‰ˆ
å¤–éƒ¨ VAD + éŸ³é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§ãƒã‚¤ã‚ºãƒ»ãƒ•ã‚£ãƒ©ãƒ¼ã‚’ã‚«ãƒƒãƒˆ
"""
from postprocess import post_process
import os
import queue
import threading
from typing import List

import numpy as np
import sounddevice as sd
import webrtcvad
from faster_whisper import WhisperModel

# ===== åŸºæœ¬è¨­å®š =====
SAMPLE_RATE = 16_000
FRAME_MS    = 30
CHUNK_MS    = 2_000
BYTES_PER_FRAME = SAMPLE_RATE * 2 * FRAME_MS // 1000
LANG = "ja"

# ===== ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ =====
BEAM_SIZE = 8
BEST_OF   = 2
TEMP      = 0.0

# ===== ãƒã‚¤ã‚º & VAD =====
MIN_LEVEL = 3_000          # ã“ã‚Œæœªæº€ã®å…¥åŠ›ã¯ç„¡è¦–
VAD_LEVEL = 0              # 0 = ã‚†ã‚‹ã„

FILLER = {
    "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
    "æœ€å¾Œã¾ã§è¦–è´ã—ã¦ãã ã•ã£ã¦ ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™",
    "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚",
}

# ===== ãƒ¢ãƒ‡ãƒ«ã¨æ¤œå‡ºå™¨ =====
model = WhisperModel("medium", device="cpu", compute_type="int8")
vad   = webrtcvad.Vad(VAD_LEVEL)

# ===== éŸ³å£°ãƒãƒƒãƒ•ã‚¡ =====
q: "queue.Queue[bytes]" = queue.Queue()


def audio_cb(indata: np.ndarray, frames: int, time, status) -> None:
    """å°ã•ã™ãã‚‹å…¥åŠ›ã¯æ¨ã¦ã‚‹"""
    if np.abs(indata).mean() * 32768 < MIN_LEVEL:
        return
    q.put(bytes(indata))


# ===== æ–‡å­—èµ·ã“ã—ã‚¹ãƒ¬ãƒƒãƒ‰ =====
def transcriber() -> None:
    buf = b""
    last_line = ""
    while True:
        buf += q.get()
        while len(buf) >= BYTES_PER_FRAME:
            frame, buf = buf[:BYTES_PER_FRAME], buf[BYTES_PER_FRAME:]

            if not vad.is_speech(frame, SAMPLE_RATE):
                continue

            # ---- CHUNK_MS ãƒŸãƒªç§’ã¶ã‚“é›†ã‚ã‚‹ ----
            speech: List[bytes] = [frame]
            while len(speech) * FRAME_MS < CHUNK_MS:
                if len(buf) < BYTES_PER_FRAME:
                    buf += q.get()
                nxt, buf = buf[:BYTES_PER_FRAME], buf[BYTES_PER_FRAME:]
                speech.append(nxt)

            # ---- æ–‡å­—èµ·ã“ã— ----
            samples = (
                np.frombuffer(b"".join(speech), np.int16)
                .astype(np.float32)
                / 32768.0
            )
            segments, _ = model.transcribe(
                samples,
                language=LANG,
                beam_size=BEAM_SIZE,
                best_of=BEST_OF,
                temperature=TEMP,
            )
            for seg in segments:
                line = seg.text.strip()
                if line in FILLER or line == last_line:
                    continue
                last_line = line
                print(line, flush=True)


# ===== ãƒã‚¤ã‚¯å…¥åŠ›ãƒ«ãƒ¼ãƒ— =====
def realtime_caption() -> None:
    threading.Thread(target=transcriber, daemon=True).start()
    with sd.RawInputStream(
        samplerate=SAMPLE_RATE,
        dtype="int16",
        channels=1,
        blocksize=0,
        callback=audio_cb,
    ):
        print("ğŸ™ï¸  è©±ã—ã¦ãã ã•ã„ (Ctrl+C ã§çµ‚äº†)")
        while True:
            sd.sleep(1_000)


# ===== ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ =====
if __name__ == "__main__":
    if os.path.exists("test.wav"):
        import soundfile as sf
        samples, _ = sf.read("test.wav", dtype="float32")
        segs, _ = model.transcribe(
            samples,
            language=LANG,
            beam_size=BEAM_SIZE,
            best_of=BEST_OF,
            temperature=TEMP,
        )
        for s in segs:
            txt = s.text.strip()
            if txt not in FILLER:
                print("ãƒ•ã‚¡ã‚¤ãƒ«çµæœ:", txt)
    else:
        realtime_caption()
