"""
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ—¥æœ¬èªæ–‡å­—èµ·ã“ã—ãƒ„ãƒ¼ãƒ« â€“ medium ãƒ¢ãƒ‡ãƒ«ç‰ˆï¼ˆ2025â€‘07â€‘17ï¼‰
-----------------------------------------------------------------
* `--wait_timeout_ms` ã§ **å›ºå®šå¾…æ©Ÿã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ** ã‚’ CLI æŒ‡å®š
* å¼•æ•°ãŒç„¡ã‘ã‚Œã°å¾“æ¥ã® `CHUNK_MS` ã¶ã‚“åé›†ï¼ˆå‹•çš„å¾…æ©Ÿãƒ­ã‚¸ãƒƒã‚¯ï¼‰
* ãƒ‡ãƒãƒƒã‚°ç”¨ã« `seg/s` ã¨ `timeout` ã‚’ stderr ã¸å‡ºåŠ›
"""

from __future__ import annotations

# â”€â”€ æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import argparse
import os
import queue
import sys
import threading
import time
from typing import List

# â”€â”€ å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import sounddevice as sd
import webrtcvad
from faster_whisper import WhisperModel
from postprocess import post_process  # å¾Œå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

# ===== CLI =====================================================
parser = argparse.ArgumentParser()
parser.add_argument("--wait_timeout_ms", type=int, default=None,
                    help="ç„¡éŸ³å¾©å¸°ã¾ã§ã®å›ºå®šå¾…æ©Ÿæ™‚é–“ï¼ˆmsï¼‰")
parser.add_argument("--device", default="cpu", choices=["cpu", "cuda", "auto"],
                    help="æ¨è«–ãƒ‡ãƒã‚¤ã‚¹")
parser.add_argument("--compute_type", default="int8",
                    choices=["int8", "int16", "float16", "float32"],
                    help="é‡å­åŒ–ç²¾åº¦")
parser.add_argument("--debug", action="store_true")
args = parser.parse_args()

# ===== åŸºæœ¬è¨­å®š =================================================
SAMPLE_RATE = 16_000            # 16Â kHz
FRAME_MS    = 30                # VAD åˆ¤å®šãƒ•ãƒ¬ãƒ¼ãƒ é•·
CHUNK_MS    = 2_000             # å¾“æ¥ã®æœ€å¤§ãƒãƒ£ãƒ³ã‚¯
WAIT_TIMEOUT_MS: int | None = args.wait_timeout_ms
if args.debug:
    print(f"DEBUG timeout = {WAIT_TIMEOUT_MS}", file=sys.stderr)

BYTES_PER_FRAME = SAMPLE_RATE * 2 * FRAME_MS // 1000  # 16â€‘bitÂ mono
LANG = "ja"

# ===== ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ========================================
BEAM_SIZE = 8
BEST_OF   = 2
TEMP      = 0.0

# ===== ãƒã‚¤ã‚º/VAD é–¾å€¤ =========================================
MIN_LEVEL = 3_000  # å…¥åŠ›æŒ¯å¹…ãŒã“ã‚Œæœªæº€ãªã‚‰é®æ–­
VAD_LEVEL = 0      # 0=ã‚†ã‚‹ã„, 3=å³æ ¼

FILLER = {
    "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
    "æœ€å¾Œã¾ã§è¦–è´ã—ã¦ãã ã•ã£ã¦ ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™",
    "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚",
}

# ===== ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ ============================================
model = WhisperModel("medium", device=args.device, compute_type=args.compute_type)
vad   = webrtcvad.Vad(VAD_LEVEL)

# ===== éŸ³å£°ãƒãƒƒãƒ•ã‚¡ ============================================
aud_q: "queue.Queue[bytes]" = queue.Queue()


def audio_cb(indata: np.ndarray, frames: int, _time, _status) -> None:
    """PortAudio ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ï¼†ãƒãƒƒãƒ•ã‚¡æŠ•å…¥"""
    if np.abs(indata).mean() * 32768 < MIN_LEVEL:
        return
    aud_q.put(bytes(indata))


# ===== æ–‡å­—èµ·ã“ã—ã‚¹ãƒ¬ãƒƒãƒ‰ =====================================

def transcriber() -> None:
    buf = b""
    last_line = ""
    processed_frames = 0
    start_time = time.time()

    while True:
        buf += aud_q.get()
        while len(buf) >= BYTES_PER_FRAME:
            frame, buf = buf[:BYTES_PER_FRAME], buf[BYTES_PER_FRAME:]

            if not vad.is_speech(frame, SAMPLE_RATE):
                continue

            # ---- ã‚¹ãƒ”ãƒ¼ãƒå¡Šã‚’åé›† ----
            speech: List[bytes] = [frame]
            silence_ms = 0
            # éŸ³å£°ãŒç¶šãé™ã‚Šãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã€é€£ç¶šç„¡éŸ³ãŒ WAIT_TIMEOUT_MS ä»¥ä¸Šã§ break
            limit_ms = WAIT_TIMEOUT_MS or CHUNK_MS
            while True:
                if len(buf) < BYTES_PER_FRAME:
                    buf += aud_q.get()
                nxt, buf = buf[:BYTES_PER_FRAME], buf[BYTES_PER_FRAME:]
                speech.append(nxt)

                if vad.is_speech(nxt, SAMPLE_RATE):
                    silence_ms = 0  # éŸ³å£°ãŒæ¥ãŸã‚‰ãƒªã‚»ãƒƒãƒˆ
                else:
                    silence_ms += FRAME_MS

                # çµ‚äº†æ¡ä»¶: ç„¡éŸ³ãŒä¸€å®šæ™‚é–“ç¶šã„ãŸ ã‚‚ã—ãã¯ CHUNK_MS ä¸Šé™
                if (silence_ms >= limit_ms) or (len(speech) * FRAME_MS >= CHUNK_MS):
                    break

            # ---- Whisper æ¨è«– ----
            samples = (
                np.frombuffer(b"".join(speech), np.int16).astype(np.float32) / 32768.0
            )
            segments, _ = model.transcribe(
                samples,
                language=LANG,
                beam_size=BEAM_SIZE,
                best_of=BEST_OF,
                temperature=TEMP,
            )
            for seg in segments:
                raw = seg.text.strip()
                line = post_process(raw)
                if line in FILLER or line == last_line:
                    continue
                last_line = line
                print(line, flush=True)

            # ---- ãƒ‡ãƒãƒƒã‚°: å‡¦ç†é€Ÿåº¦ ----
            processed_frames += len(speech)
            if args.debug and processed_frames >= SAMPLE_RATE:  # æ¯ç§’è¡¨ç¤º
                elapsed = time.time() - start_time
                print(f"seg/s = {processed_frames/elapsed:.2f}", file=sys.stderr)
                processed_frames = 0
                start_time = time.time()


# ===== ãƒã‚¤ã‚¯å…¥åŠ› ==============================================

def realtime_caption() -> None:
    threading.Thread(target=transcriber, daemon=True).start()
    with sd.RawInputStream(
        samplerate=SAMPLE_RATE,
        dtype="int16",
        channels=1,
        blocksize=0,
        callback=audio_cb,
    ):
        print("ğŸ™ï¸  è©±ã—ã¦ãã ã•ã„ (Ctrl+C ã§çµ‚äº†)")
        while True:
            sd.sleep(1_000)


# ===== ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ =====================================
if __name__ == "__main__":
    if os.path.exists("test.wav"):
        import soundfile as sf

        samples, _ = sf.read("test.wav", dtype="float32")
        segs, _ = model.transcribe(
            samples,
            language=LANG,
            beam_size=BEAM_SIZE,
            best_of=BEST_OF,
            temperature=TEMP,
        )
        for s in segs:
            raw = s.text.strip()
            txt = post_process(raw)
            if txt not in FILLER:
                print("ãƒ•ã‚¡ã‚¤ãƒ«çµæœ:", txt)
    else:
        realtime_caption()
